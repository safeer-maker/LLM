{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c825408f",
   "metadata": {},
   "source": [
    "# Lets Play with DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c286eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72548477",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.path.exists(\"squad-it\")\n",
    "    print (\"Dataset exists\")\n",
    "except:\n",
    "    !git clone https://github.com/crux82/squad-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad747ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir squad-it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381672fe",
   "metadata": {},
   "source": [
    "#### Unzip the dataset files manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "squad_it_data =  load_dataset ('json',data_files=\"squad-it\\\\SQuAD_it-train.json\", field = 'data')\n",
    "squad_it_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc650892",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_it_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ef6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = {'train': \"squad-it\\\\SQuAD_it-train.json\", \"test\" : \"squad-it\\\\SQuAD_it-test.json\"}\n",
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d751554",
   "metadata": {},
   "outputs": [],
   "source": [
    "squid_comp_dataset = load_dataset('json', data_files= data_file, field = 'data')\n",
    "squid_comp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "squid_comp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e89ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = {'train': \"squad-it\\\\SQuAD_it-train.json.gz\", \"test\" : \"squad-it\\\\SQuAD_it-test.json.gz\"}\n",
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd723dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_comp_dataset = load_dataset('json', data_files= data_file, field = 'data')\n",
    "squad_comp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7c8f0",
   "metadata": {},
   "source": [
    "# Working on UCI Drug Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f72007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "drd_dataset_orig = load_dataset('csv', data_files = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\" , delimiter = '\\t' )\n",
    "drd_dataset = drd_dataset_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset ['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60307805",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_rand = drd_dataset['train'].shuffle(seed = 41).select(range(1000))\n",
    "drd_dataset_rand[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9debf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset['train'].unique('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in drd_dataset.keys():\n",
    "    assert len(drd_dataset[split]) == len (drd_dataset[split].unique(\"Unnamed: 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(drd_dataset[split]) == len (drd_dataset[split].unique(\"Unnamed: 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed5027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70bf6803",
   "metadata": {},
   "source": [
    "def lower_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset = drd_dataset.rename_column(original_column_name=\"Unnamed: 0\", new_column_name='patient_id')\n",
    "drd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "len (drd_dataset['train'].unique('drugName') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d96db0",
   "metadata": {},
   "source": [
    "### convertint the conditon colomn to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fbe249",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset['train'][0]['condition'] = \"Sam is here\"\n",
    "drd_dataset['train']['condition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = drd_dataset['train'][0]['condition'].lower()\n",
    "sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230dca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset['train']['condition'][0] = drd_dataset['train']['condition'][0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset['train']['condition'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631fcd6",
   "metadata": {},
   "source": [
    "The data is not converting to lower case with these two approches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(example):\n",
    "    return {'condition': example['condition'].lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_cp = drd_dataset_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80f7e8",
   "metadata": {},
   "source": [
    "drd_dataset_cp.map (lower_case)\n",
    "drd_dataset_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb008a6",
   "metadata": {},
   "source": [
    "Checking for null values in condition column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_uniq_np = np.array( drd_dataset_cp['train']['condition'] )\n",
    "cond_uniq_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_uniq_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be13e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "len (cond_uniq_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a345cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type (cond_uniq_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab3aa1",
   "metadata": {},
   "source": [
    "np.isnan(cond_uniq_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da72de",
   "metadata": {},
   "source": [
    "np.unique(cond_uniq_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e39c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_cp['train']['condition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ccf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(lambda x , y : x + y )(9 , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_nl =  drd_dataset_cp.filter(lambda x: x[\"condition\"] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a951f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_nl['train']['condition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_nl_low = drd_dataset_nl.map(lower_case)\n",
    "drd_dataset_nl_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_nl_low['train']['condition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len (cond_uniq_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_cp.filter ( lambda x : x['condition'] is not None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee319f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf042bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_review_len(dataset):\n",
    "    return {'review_len': len (dataset[\"review\"].split())}\n",
    "\n",
    "drd_dataset_new_col = drd_dataset_nl_low.map(compute_review_len)\n",
    "drd_dataset_new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_new_col['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68902bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_new_col['train'].sort('review_len')[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_h_rn = drd_dataset_new_col.filter (lambda x: x['review_len'] > 30 )\n",
    "drd_dataset_h_rn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a807b862",
   "metadata": {},
   "source": [
    "drd_dataset_h_rn[\"train\"].sort(\"review_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475eb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_h_rn[\"train\"][\"review_len\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d843f",
   "metadata": {},
   "source": [
    "# Removing HTML characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0eb937",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I&#039;m a transformer called BERT\"\n",
    "html.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_final = drd_dataset_h_rn.map(lambda x : {'review': html.unescape(x['review'])})\n",
    "drd_dataset_final['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_final = drd_dataset_final.map(lambda x : {'review' : x['review'].replace (\"\\\\'\",'\\'')  })\n",
    "drd_dataset_final['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_final_map = drd_dataset_final.map(\n",
    "    lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91018de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = drd_dataset_final_map['train'].set_format('pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_final_map['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f15967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = drd_dataset_final_map['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.rename_column(\"Unnamed: 0\", \"patient_id\")\n",
    "df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = (\n",
    "    df_data[\"condition\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n",
    ")\n",
    "frequencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ee6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ffa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.reset_format()\n",
    "df_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28830d8b",
   "metadata": {},
   "source": [
    "# Working with large Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46731e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db8f2f",
   "metadata": {},
   "source": [
    "The Pebmed dataset is very large about. but no longer hosted by https://the-eye.eu "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fde39",
   "metadata": {},
   "source": [
    "data_file_url = \"https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_1_time1625689368_default.jsonl.zst\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc5aff",
   "metadata": {},
   "source": [
    "pubmed_dataset = load_dataset('json', data_files = data_file_url, field = 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58744b7c",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\n",
    "data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "pubmed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cee4d",
   "metadata": {},
   "source": [
    "So I am going to work with the drug dataset keeping only patient Ids and reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files= \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "drd_data = load_dataset ('csv', data_files=data_files, delimiter = '\\t')#,streaming = True )\n",
    "drd_data = drd_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcacc5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_data = drd_data.rename_column(original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\")\n",
    "drd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_data.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_data_new = drd_data.select_columns(['patient_id','review'])\n",
    "drd_data_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efacff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "html.unescape(drd_data_new[0]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_nohtml = drd_data_new.map(lambda x : {\"review\": html.unescape(x['review']) })\n",
    "drd_nohtml[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba8ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_dataset_final = drd_data_new.map(lambda x : {'review': html.unescape(x['review'])})\n",
    "drd_dataset_final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b41275",
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_nohtml['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc440c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "classifier = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a36a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenizer( drd_nohtml['review'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = drd_nohtml.map(lambda x : tokenizer(x['review']), batched=True)\n",
    "token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "law_dataset_streamed = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "# next(iter(law_dataset_streamed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ae79e",
   "metadata": {},
   "source": [
    "# Creating your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0011b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (response)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b942a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876bb67",
   "metadata": {},
   "source": [
    "using dotenv to get git user access tocken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"GIT_TOCKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627273b2",
   "metadata": {},
   "source": [
    "Its better to use os.env variable to get the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GITHUB_TOKEN = os.environ.get('GIT_TOCKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbddead",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a0e93",
   "metadata": {},
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a39411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "GITHUB_TOKEN = os.environ.get('GIT_TOCKEN')\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10_000,\n",
    "    rate_limit=5_000,\n",
    "    issues_path=Path(\".\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            break\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7b00b",
   "metadata": {},
   "source": [
    "Breaking the fetcher funciton to the point where github rate limit exceeds.\n",
    "Avoiding to wait for one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64109283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset = load_dataset ('json', data_files=\"datasets-issues-sub.json\", split='train')\n",
    "get_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "git_df = pd.DataFrame(get_dataset)\n",
    "git_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf4e30",
   "metadata": {},
   "source": [
    "Removing the rows which are just the pull request not the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset['pull_request'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9763615",
   "metadata": {},
   "outputs": [],
   "source": [
    "(git_df['pull_request'] == None ).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(git_df['pull_request'] == None).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e93e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset.filter(lambda x : x['pull_request'] is None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_data_pull_request =  get_dataset.map (lambda x: {'is_pull_request' : x['pull_request'] != None })\n",
    "\n",
    "git_data_pull_request[\"is_pull_request\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39782566",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_data_pull_request.set_format('pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_df[[\"created_at\",\"closed_at\"]][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f42ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(git_data_pull_request[\"closed_at\"][1] - git_data_pull_request[\"created_at\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5afc7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_data_pull_request[\"closed_at\"][0]  is pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_data_pull_request_com_avg = git_data_pull_request.filter(lambda x : ( x[\"closed_at\"] is not pd.NaT ) and ( x['created_at'] is not pd.NaT ) )\n",
    "git_data_pull_request_com_avg['closed_at'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_data_pull_request_com_avg['body'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73cf87",
   "metadata": {},
   "source": [
    "# Creating a Sementic Search with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28eec067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason'],\n",
       "    num_rows: 2059\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "git_data = load_dataset ('json', data_files='datasets-issues-sub.json', split='train')\n",
    "git_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad98d6",
   "metadata": {},
   "source": [
    "PreProcessing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "233ff541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6215',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6215',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/6215.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/6215.patch',\n",
       "  'merged_at': None},\n",
       " None,\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6213',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6213',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/6213.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/6213.patch',\n",
       "  'merged_at': None},\n",
       " None,\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6211',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6211',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/6211.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/6211.patch',\n",
       "  'merged_at': datetime.datetime(2023, 9, 4, 14, 47, 17)},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6210',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6210',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/6210.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/6210.patch',\n",
       "  'merged_at': datetime.datetime(2023, 9, 4, 7, 30)},\n",
       " None,\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6208',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6208',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/6208.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/6208.patch',\n",
       "  'merged_at': datetime.datetime(2023, 9, 4, 9, 13, 32)},\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_data[\"pull_request\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "733415e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6215, 6214, 6213, 6212, 6211, 6210, 6209, 6208, 6207, 6206]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_data[\"number\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7f23f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None in git_data[\"body\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66dad2",
   "metadata": {},
   "source": [
    "cleaning the pull requests and the body that does not have any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6b29d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6215',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6215',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/6215.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/6215.patch',\n",
       "  'merged_at': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6213',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6213',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/6213.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/6213.patch',\n",
       "  'merged_at': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6211',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6211',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/6211.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/6211.patch',\n",
       "  'merged_at': datetime.datetime(2023, 9, 4, 14, 47, 17)},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6210',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6210',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/6210.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/6210.patch',\n",
       "  'merged_at': datetime.datetime(2023, 9, 4, 7, 30)},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6208',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6208',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/6208.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/6208.patch',\n",
       "  'merged_at': datetime.datetime(2023, 9, 4, 9, 13, 32)}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_data_no_pull_req = git_data.filter(lambda x : (x[\"pull_request\"] is not None) and (x['body'] is not None) )\n",
    "git_data_no_pull_req[\"pull_request\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc8adf",
   "metadata": {},
   "source": [
    "Removing unused Cloumns from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0004fdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'body', 'html_url', 'comments', 'number'],\n",
       "    num_rows: 928\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_colums = [\"title\", 'body','html_url','comments', 'number']\n",
    "git_data_sel_columns =  git_data_no_pull_req.select_columns (column_names=selected_colums)\n",
    "git_data_sel_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f208fd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't ignore results of pattern resolving if `self.data_files` is not None. Otherwise lines 854 and 1037 make no sense.\",\n",
       " 'Use [`array.flatten`](https://arrow.apache.org/docs/python/generated/pyarrow.ListArray.html#pyarrow.ListArray.flatten) that takes `.offset` into account instead of `array.values` in array cast/embed.',\n",
       " 'If a split is empty, then the JSON split info should mention num_bytes = 0 and num_examples = 0.\\r\\n\\r\\nUntil now they were omited because the JSON dumps ignore the fields that are equal to the default values.\\r\\n\\r\\nThis is needed in datasets-server since we parse this information to the viewer',\n",
       " 'Temporarily pin fsspec < 2023.9.0 until permanent solution is found.\\r\\n\\r\\nHot fix #6209.',\n",
       " 'This PR is a hotfix of:\\r\\n- #6207\\r\\n\\r\\nThat PR introduced the filtering out of `.zip` extensions. This PR reverts that.\\r\\n\\r\\nHot fix #6207.\\r\\n\\r\\nMaybe we should do patch releases: the bug was introduced in 2.13.1.\\r\\n\\r\\nCC: @lhoestq ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_data_sel_columns['body'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72cc44f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['Fix checking patterns to infer packaged builder ',\n",
       "  'Better list array values handling in cast/embed storage',\n",
       "  'Fix empty splitinfo json'],\n",
       " 'body': [\"Don't ignore results of pattern resolving if `self.data_files` is not None. Otherwise lines 854 and 1037 make no sense.\",\n",
       "  'Use [`array.flatten`](https://arrow.apache.org/docs/python/generated/pyarrow.ListArray.html#pyarrow.ListArray.flatten) that takes `.offset` into account instead of `array.values` in array cast/embed.',\n",
       "  'If a split is empty, then the JSON split info should mention num_bytes = 0 and num_examples = 0.\\r\\n\\r\\nUntil now they were omited because the JSON dumps ignore the fields that are equal to the default values.\\r\\n\\r\\nThis is needed in datasets-server since we parse this information to the viewer'],\n",
       " 'html_url': ['https://github.com/huggingface/datasets/pull/6215',\n",
       "  'https://github.com/huggingface/datasets/pull/6213',\n",
       "  'https://github.com/huggingface/datasets/pull/6211'],\n",
       " 'comments': [2, 4, 4],\n",
       " 'number': [6215, 6213, 6211]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_data_sel_columns[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad73cbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>comments</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fix checking patterns to infer packaged builder</td>\n",
       "      <td>Don't ignore results of pattern resolving if `...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6215</td>\n",
       "      <td>2</td>\n",
       "      <td>6215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Better list array values handling in cast/embe...</td>\n",
       "      <td>Use [`array.flatten`](https://arrow.apache.org...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6213</td>\n",
       "      <td>4</td>\n",
       "      <td>6213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fix empty splitinfo json</td>\n",
       "      <td>If a split is empty, then the JSON split info ...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6211</td>\n",
       "      <td>4</td>\n",
       "      <td>6211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Temporarily pin fsspec &lt; 2023.9.0</td>\n",
       "      <td>Temporarily pin fsspec &lt; 2023.9.0 until perman...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6210</td>\n",
       "      <td>3</td>\n",
       "      <td>6210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do not filter out .zip extensions from no-scri...</td>\n",
       "      <td>This PR is a hotfix of:\\r\\n- #6207\\r\\n\\r\\nThat...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6208</td>\n",
       "      <td>6</td>\n",
       "      <td>6208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>[Minor edit] Fix typo in class name</td>\n",
       "      <td>Typo: `datasets.DatsetDict` -&gt; `datasets.Datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/4207</td>\n",
       "      <td>0</td>\n",
       "      <td>4207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Add Nerval Metric</td>\n",
       "      <td>This PR adds readme.md and ner_val.py to metri...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/4206</td>\n",
       "      <td>1</td>\n",
       "      <td>4206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Fix `convert_file_size_to_int` for kilobits an...</td>\n",
       "      <td>Minor change to fully align this function with...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/4205</td>\n",
       "      <td>1</td>\n",
       "      <td>4205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>Add Recall Metric Card</td>\n",
       "      <td>What this PR mainly does:\\r\\n- add metric card...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/4204</td>\n",
       "      <td>2</td>\n",
       "      <td>4204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>Add Precision Metric Card</td>\n",
       "      <td>What this PR mainly does:\\r\\n- add metric card...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/4203</td>\n",
       "      <td>1</td>\n",
       "      <td>4203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  ... number\n",
       "0     Fix checking patterns to infer packaged builder   ...   6215\n",
       "1    Better list array values handling in cast/embe...  ...   6213\n",
       "2                             Fix empty splitinfo json  ...   6211\n",
       "3                    Temporarily pin fsspec < 2023.9.0  ...   6210\n",
       "4    Do not filter out .zip extensions from no-scri...  ...   6208\n",
       "..                                                 ...  ...    ...\n",
       "923                [Minor edit] Fix typo in class name  ...   4207\n",
       "924                                  Add Nerval Metric  ...   4206\n",
       "925  Fix `convert_file_size_to_int` for kilobits an...  ...   4205\n",
       "926                             Add Recall Metric Card  ...   4204\n",
       "927                          Add Precision Metric Card  ...   4203\n",
       "\n",
       "[928 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_df = git_data_sel_columns\n",
    "git_df.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba892ee",
   "metadata": {},
   "source": [
    "Getting all comments from that specific issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1be65189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/1706821141',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6215#issuecomment-1706821141',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/6215',\n",
       "  'id': 1706821141,\n",
       "  'node_id': 'IC_kwDODunzps5lvAYV',\n",
       "  'user': {'login': 'HuggingFaceDocBuilderDev',\n",
       "   'id': 99929124,\n",
       "   'node_id': 'U_kgDOBfTMJA',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/99929124?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/HuggingFaceDocBuilderDev',\n",
       "   'html_url': 'https://github.com/HuggingFaceDocBuilderDev',\n",
       "   'followers_url': 'https://api.github.com/users/HuggingFaceDocBuilderDev/followers',\n",
       "   'following_url': 'https://api.github.com/users/HuggingFaceDocBuilderDev/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/HuggingFaceDocBuilderDev/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/HuggingFaceDocBuilderDev/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/HuggingFaceDocBuilderDev/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/HuggingFaceDocBuilderDev/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/HuggingFaceDocBuilderDev/repos',\n",
       "   'events_url': 'https://api.github.com/users/HuggingFaceDocBuilderDev/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/HuggingFaceDocBuilderDev/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2023-09-05T15:17:30Z',\n",
       "  'updated_at': '2023-09-06T10:25:33Z',\n",
       "  'author_association': 'NONE',\n",
       "  'body': '_The documentation is not available anymore as the PR was closed or merged._',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/1706821141/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/1707040617',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6215#issuecomment-1707040617',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/6215',\n",
       "  'id': 1707040617,\n",
       "  'node_id': 'IC_kwDODunzps5lv19p',\n",
       "  'user': {'login': 'lhoestq',\n",
       "   'id': 42851186,\n",
       "   'node_id': 'MDQ6VXNlcjQyODUxMTg2',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/42851186?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/lhoestq',\n",
       "   'html_url': 'https://github.com/lhoestq',\n",
       "   'followers_url': 'https://api.github.com/users/lhoestq/followers',\n",
       "   'following_url': 'https://api.github.com/users/lhoestq/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/lhoestq/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/lhoestq/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/lhoestq/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/lhoestq/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/lhoestq/repos',\n",
       "   'events_url': 'https://api.github.com/users/lhoestq/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/lhoestq/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2023-09-05T17:42:53Z',\n",
       "  'updated_at': '2023-09-05T17:42:53Z',\n",
       "  'author_association': 'MEMBER',\n",
       "  'body': 'oh wow good catch',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/1707040617/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/1708090333',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6215#issuecomment-1708090333',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/6215',\n",
       "  'id': 1708090333,\n",
       "  'node_id': 'IC_kwDODunzps5lz2Pd',\n",
       "  'user': {'login': 'github-actions[bot]',\n",
       "   'id': 41898282,\n",
       "   'node_id': 'MDM6Qm90NDE4OTgyODI=',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/in/15368?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/github-actions%5Bbot%5D',\n",
       "   'html_url': 'https://github.com/apps/github-actions',\n",
       "   'followers_url': 'https://api.github.com/users/github-actions%5Bbot%5D/followers',\n",
       "   'following_url': 'https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/github-actions%5Bbot%5D/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/github-actions%5Bbot%5D/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/github-actions%5Bbot%5D/repos',\n",
       "   'events_url': 'https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/github-actions%5Bbot%5D/received_events',\n",
       "   'type': 'Bot',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2023-09-06T10:33:59Z',\n",
       "  'updated_at': '2023-09-06T10:33:59Z',\n",
       "  'author_association': 'NONE',\n",
       "  'body': '<details>\\n<summary>Show benchmarks</summary>\\n\\nPyArrow==8.0.0\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.006681 / 0.011353 (-0.004672) | 0.003967 / 0.011008 (-0.007041) | 0.085590 / 0.038508 (0.047082) | 0.079285 / 0.023109 (0.056176) | 0.311583 / 0.275898 (0.035685) | 0.345578 / 0.323480 (0.022098) | 0.004115 / 0.007986 (-0.003871) | 0.004286 / 0.004328 (-0.000043) | 0.064405 / 0.004250 (0.060155) | 0.055084 / 0.037052 (0.018032) | 0.316117 / 0.258489 (0.057628) | 0.354737 / 0.293841 (0.060896) | 0.031280 / 0.128546 (-0.097266) | 0.008395 / 0.075646 (-0.067251) | 0.288910 / 0.419271 (-0.130362) | 0.051291 / 0.043533 (0.007759) | 0.309125 / 0.255139 (0.053986) | 0.349673 / 0.283200 (0.066473) | 0.025016 / 0.141683 (-0.116667) | 1.475577 / 1.452155 (0.023422) | 1.558967 / 1.492716 (0.066251) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.208504 / 0.018006 (0.190498) | 0.462270 / 0.000490 (0.461780) | 0.003476 / 0.000200 (0.003276) | 0.000073 / 0.000054 (0.000018) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.030371 / 0.037411 (-0.007041) | 0.086157 / 0.014526 (0.071631) | 0.098162 / 0.176557 (-0.078395) | 0.154649 / 0.737135 (-0.582486) | 0.098697 / 0.296338 (-0.197642) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.405883 / 0.215209 (0.190674) | 4.049614 / 2.077655 (1.971959) | 2.075047 / 1.504120 (0.570927) | 1.917782 / 1.541195 (0.376587) | 2.030268 / 1.468490 (0.561778) | 0.483974 / 4.584777 (-4.100803) | 3.542147 / 3.745712 (-0.203566) | 3.305999 / 5.269862 (-1.963863) | 2.052287 / 4.565676 (-2.513390) | 0.057246 / 0.424275 (-0.367029) | 0.007631 / 0.007607 (0.000024) | 0.488189 / 0.226044 (0.262144) | 4.884784 / 2.268929 (2.615856) | 2.576304 / 55.444624 (-52.868320) | 2.241249 / 6.876477 (-4.635228) | 2.490512 / 2.142072 (0.348440) | 0.584495 / 4.805227 (-4.220733) | 0.134741 / 6.500664 (-6.365923) | 0.061639 / 0.075469 (-0.013830) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 1.317717 / 1.841788 (-0.524071) | 20.098594 / 8.074308 (12.024286) | 14.641051 / 10.191392 (4.449659) | 0.165291 / 0.680424 (-0.515133) | 0.019179 / 0.534201 (-0.515022) | 0.399506 / 0.579283 (-0.179777) | 0.407662 / 0.434364 (-0.026701) | 0.457965 / 0.540337 (-0.082372) | 0.626401 / 1.386936 (-0.760536) |\\n\\n</details>\\nPyArrow==latest\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.007076 / 0.011353 (-0.004277) | 0.004125 / 0.011008 (-0.006884) | 0.064861 / 0.038508 (0.026353) | 0.082390 / 0.023109 (0.059281) | 0.423227 / 0.275898 (0.147329) | 0.452229 / 0.323480 (0.128750) | 0.005594 / 0.007986 (-0.002392) | 0.003465 / 0.004328 (-0.000863) | 0.064661 / 0.004250 (0.060411) | 0.057945 / 0.037052 (0.020892) | 0.424572 / 0.258489 (0.166083) | 0.465349 / 0.293841 (0.171509) | 0.032687 / 0.128546 (-0.095859) | 0.008573 / 0.075646 (-0.067074) | 0.073020 / 0.419271 (-0.346251) | 0.048423 / 0.043533 (0.004891) | 0.413425 / 0.255139 (0.158286) | 0.433778 / 0.283200 (0.150578) | 0.023942 / 0.141683 (-0.117741) | 1.495190 / 1.452155 (0.043036) | 1.586526 / 1.492716 (0.093810) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.271805 / 0.018006 (0.253799) | 0.454922 / 0.000490 (0.454432) | 0.015386 / 0.000200 (0.015186) | 0.000129 / 0.000054 (0.000074) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.033804 / 0.037411 (-0.003607) | 0.099317 / 0.014526 (0.084791) | 0.107207 / 0.176557 (-0.069349) | 0.160926 / 0.737135 (-0.576210) | 0.108669 / 0.296338 (-0.187670) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.430776 / 0.215209 (0.215567) | 4.297622 / 2.077655 (2.219967) | 2.285918 / 1.504120 (0.781798) | 2.109608 / 1.541195 (0.568413) | 2.208326 / 1.468490 (0.739836) | 0.490016 / 4.584777 (-4.094761) | 3.570609 / 3.745712 (-0.175103) | 3.406335 / 5.269862 (-1.863526) | 2.070664 / 4.565676 (-2.495012) | 0.058089 / 0.424275 (-0.366186) | 0.007425 / 0.007607 (-0.000182) | 0.506972 / 0.226044 (0.280927) | 5.078643 / 2.268929 (2.809714) | 2.858973 / 55.444624 (-52.585651) | 2.457344 / 6.876477 (-4.419132) | 2.687727 / 2.142072 (0.545654) | 0.592134 / 4.805227 (-4.213093) | 0.133966 / 6.500664 (-6.366698) | 0.061800 / 0.075469 (-0.013669) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 1.337167 / 1.841788 (-0.504620) | 20.743951 / 8.074308 (12.669643) | 15.402686 / 10.191392 (5.211294) | 0.164548 / 0.680424 (-0.515876) | 0.020244 / 0.534201 (-0.513957) | 0.399044 / 0.579283 (-0.180239) | 0.414036 / 0.434364 (-0.020328) | 0.474141 / 0.540337 (-0.066197) | 0.654455 / 1.386936 (-0.732482) |\\n\\n</details>\\n</details>\\n\\n![](https://cml.dev/watermark.png#4de930c45a81a6dff1805bf45f59170e9f953eeb \"CML watermark\")\\n',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/1708090333/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "git_key = os.environ.get (\"GIT_TOCKEN\")\n",
    "git_header = {\"Authorization\": f\"token {git_key}\"}\n",
    "\n",
    "issue_number = 6215\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "\n",
    "data = requests.get (url=url, headers=git_header).json()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac34a30c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_json \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mjson()\n\u001b[0;32m      2\u001b[0m data_json[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'json'"
     ]
    }
   ],
   "source": [
    "data_json = data.json()\n",
    "data_json[0]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb980857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments (issue_number):\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    data = requests.get (url=url, headers=git_header).json()\n",
    "    return [r['body'] for r in data]\n",
    "\n",
    "\n",
    "get_comments(6211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cd711",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_issues_with_comments = git_df.map(lambda x : {'comments_data' : get_comments(x['number'])})\n",
    "git_issues_with_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_issues_with_comments['comments_data'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_issues_with_comments.save_to_disk(\"git_dataset_with_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_df = git_issues_with_comments.to_pandas()\n",
    "git_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_df_cmments_explode = git_df.explode(\"comments_data\", ignore_index=True )\n",
    "git_df_cmments_explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "git_database_commented = Dataset.from_pandas (git_df_cmments_explode)\n",
    "\n",
    "git_database_commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802794ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_database_commented['comments_data'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_database_commented = git_database_commented.map (lambda x: {'comment_count' : 0 if x[\"comments_data\"] is None else len(x[\"comments_data\"].split()) } )\n",
    "git_database_commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b78c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_database_commented['comment_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_database_commented = git_database_commented.filter (lambda x : x[\"comment_count\"] > 15 )\n",
    "git_database_commented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ff377",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_database_commented [:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d56cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4f6b7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ccc186e7ec41cc8018272925f27ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "git_dataset = Dataset.load_from_disk (\"git_dataset_with_comments\")\n",
    "git_dataset = git_dataset.to_pandas().explode(\"comments_data\", ignore_index=True)\n",
    "git_dataset = Dataset.from_pandas(git_dataset)\n",
    "git_dataset.save_to_disk(\"git_dataset_parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd0cdde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load_from_disk(\"git_dataset_parsed\")\n",
    "dataset_df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d430692b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'body', 'html_url', 'comments', 'number', 'comments_data'],\n",
       "    num_rows: 2912\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_nnul = dataset.filter (lambda x : x['comments_data'] is not None)\n",
    "dataset_nnul "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2572519f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad8ff6e8262471f99c75eca31a53068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2912 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'body', 'html_url', 'comments', 'number', 'comments_data', 'comments_count'],\n",
       "    num_rows: 2912\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_count = dataset_nnul.map (lambda x : {\"comments_count\" : len(x['comments_data'].split())} )\n",
    "dataset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb61e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>comments</th>\n",
       "      <th>number</th>\n",
       "      <th>comments_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fix checking patterns to infer packaged builder</td>\n",
       "      <td>Don't ignore results of pattern resolving if `...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6215</td>\n",
       "      <td>2</td>\n",
       "      <td>6215</td>\n",
       "      <td>_The documentation is not available anymore as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fix checking patterns to infer packaged builder</td>\n",
       "      <td>Don't ignore results of pattern resolving if `...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6215</td>\n",
       "      <td>2</td>\n",
       "      <td>6215</td>\n",
       "      <td>oh wow good catch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fix checking patterns to infer packaged builder</td>\n",
       "      <td>Don't ignore results of pattern resolving if `...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6215</td>\n",
       "      <td>2</td>\n",
       "      <td>6215</td>\n",
       "      <td>&lt;details&gt;\\n&lt;summary&gt;Show benchmarks&lt;/summary&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Better list array values handling in cast/embe...</td>\n",
       "      <td>Use [`array.flatten`](https://arrow.apache.org...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6213</td>\n",
       "      <td>4</td>\n",
       "      <td>6213</td>\n",
       "      <td>&lt;details&gt;\\n&lt;summary&gt;Show benchmarks&lt;/summary&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Better list array values handling in cast/embe...</td>\n",
       "      <td>Use [`array.flatten`](https://arrow.apache.org...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6213</td>\n",
       "      <td>4</td>\n",
       "      <td>6213</td>\n",
       "      <td>&lt;details&gt;\\n&lt;summary&gt;Show benchmarks&lt;/summary&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>Add Nerval Metric</td>\n",
       "      <td>This PR adds readme.md and ner_val.py to metri...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/4206</td>\n",
       "      <td>1</td>\n",
       "      <td>4206</td>\n",
       "      <td>Metrics are deprecated in `datasets` and `eval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>Fix `convert_file_size_to_int` for kilobits an...</td>\n",
       "      <td>Minor change to fully align this function with...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/4205</td>\n",
       "      <td>1</td>\n",
       "      <td>4205</td>\n",
       "      <td>_The documentation is not available anymore as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>Add Recall Metric Card</td>\n",
       "      <td>What this PR mainly does:\\r\\n- add metric card...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/4204</td>\n",
       "      <td>2</td>\n",
       "      <td>4204</td>\n",
       "      <td>_The documentation is not available anymore as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>Add Recall Metric Card</td>\n",
       "      <td>What this PR mainly does:\\r\\n- add metric card...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/4204</td>\n",
       "      <td>2</td>\n",
       "      <td>4204</td>\n",
       "      <td>This looks good to me!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>Add Precision Metric Card</td>\n",
       "      <td>What this PR mainly does:\\r\\n- add metric card...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/4203</td>\n",
       "      <td>1</td>\n",
       "      <td>4203</td>\n",
       "      <td>_The documentation is not available anymore as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2912 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  ...                                      comments_data\n",
       "0      Fix checking patterns to infer packaged builder   ...  _The documentation is not available anymore as...\n",
       "1      Fix checking patterns to infer packaged builder   ...                                  oh wow good catch\n",
       "2      Fix checking patterns to infer packaged builder   ...  <details>\\n<summary>Show benchmarks</summary>\\...\n",
       "3     Better list array values handling in cast/embe...  ...  <details>\\n<summary>Show benchmarks</summary>\\...\n",
       "4     Better list array values handling in cast/embe...  ...  <details>\\n<summary>Show benchmarks</summary>\\...\n",
       "...                                                 ...  ...                                                ...\n",
       "2940                                  Add Nerval Metric  ...  Metrics are deprecated in `datasets` and `eval...\n",
       "2941  Fix `convert_file_size_to_int` for kilobits an...  ...  _The documentation is not available anymore as...\n",
       "2942                             Add Recall Metric Card  ...  _The documentation is not available anymore as...\n",
       "2943                             Add Recall Metric Card  ...                            This looks good to me! \n",
       "2944                          Add Precision Metric Card  ...  _The documentation is not available anymore as...\n",
       "\n",
       "[2912 rows x 6 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df_nna = dataset_df.dropna()\n",
    "dataset_df_nna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "801be621",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_count = dataset_count.filter(lambda x: x['comments_count'] > 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b195b1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<details>\\n<summary>Show benchmarks</summary>\\n\\nPyArrow==8.0.0\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.006681 / 0.011353 (-0.004672) | 0.003967 / 0.011008 (-0.007041) | 0.085590 / 0.038508 (0.047082) | 0.079285 / 0.023109 (0.056176) | 0.311583 / 0.275898 (0.035685) | 0.345578 / 0.323480 (0.022098) | 0.004115 / 0.007986 (-0.003871) | 0.004286 / 0.004328 (-0.000043) | 0.064405 / 0.004250 (0.060155) | 0.055084 / 0.037052 (0.018032) | 0.316117 / 0.258489 (0.057628) | 0.354737 / 0.293841 (0.060896) | 0.031280 / 0.128546 (-0.097266) | 0.008395 / 0.075646 (-0.067251) | 0.288910 / 0.419271 (-0.130362) | 0.051291 / 0.043533 (0.007759) | 0.309125 / 0.255139 (0.053986) | 0.349673 / 0.283200 (0.066473) | 0.025016 / 0.141683 (-0.116667) | 1.475577 / 1.452155 (0.023422) | 1.558967 / 1.492716 (0.066251) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.208504 / 0.018006 (0.190498) | 0.462270 / 0.000490 (0.461780) | 0.003476 / 0.000200 (0.003276) | 0.000073 / 0.000054 (0.000018) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.030371 / 0.037411 (-0.007041) | 0.086157 / 0.014526 (0.071631) | 0.098162 / 0.176557 (-0.078395) | 0.154649 / 0.737135 (-0.582486) | 0.098697 / 0.296338 (-0.197642) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.405883 / 0.215209 (0.190674) | 4.049614 / 2.077655 (1.971959) | 2.075047 / 1.504120 (0.570927) | 1.917782 / 1.541195 (0.376587) | 2.030268 / 1.468490 (0.561778) | 0.483974 / 4.584777 (-4.100803) | 3.542147 / 3.745712 (-0.203566) | 3.305999 / 5.269862 (-1.963863) | 2.052287 / 4.565676 (-2.513390) | 0.057246 / 0.424275 (-0.367029) | 0.007631 / 0.007607 (0.000024) | 0.488189 / 0.226044 (0.262144) | 4.884784 / 2.268929 (2.615856) | 2.576304 / 55.444624 (-52.868320) | 2.241249 / 6.876477 (-4.635228) | 2.490512 / 2.142072 (0.348440) | 0.584495 / 4.805227 (-4.220733) | 0.134741 / 6.500664 (-6.365923) | 0.061639 / 0.075469 (-0.013830) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 1.317717 / 1.841788 (-0.524071) | 20.098594 / 8.074308 (12.024286) | 14.641051 / 10.191392 (4.449659) | 0.165291 / 0.680424 (-0.515133) | 0.019179 / 0.534201 (-0.515022) | 0.399506 / 0.579283 (-0.179777) | 0.407662 / 0.434364 (-0.026701) | 0.457965 / 0.540337 (-0.082372) | 0.626401 / 1.386936 (-0.760536) |\\n\\n</details>\\nPyArrow==latest\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.007076 / 0.011353 (-0.004277) | 0.004125 / 0.011008 (-0.006884) | 0.064861 / 0.038508 (0.026353) | 0.082390 / 0.023109 (0.059281) | 0.423227 / 0.275898 (0.147329) | 0.452229 / 0.323480 (0.128750) | 0.005594 / 0.007986 (-0.002392) | 0.003465 / 0.004328 (-0.000863) | 0.064661 / 0.004250 (0.060411) | 0.057945 / 0.037052 (0.020892) | 0.424572 / 0.258489 (0.166083) | 0.465349 / 0.293841 (0.171509) | 0.032687 / 0.128546 (-0.095859) | 0.008573 / 0.075646 (-0.067074) | 0.073020 / 0.419271 (-0.346251) | 0.048423 / 0.043533 (0.004891) | 0.413425 / 0.255139 (0.158286) | 0.433778 / 0.283200 (0.150578) | 0.023942 / 0.141683 (-0.117741) | 1.495190 / 1.452155 (0.043036) | 1.586526 / 1.492716 (0.093810) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.271805 / 0.018006 (0.253799) | 0.454922 / 0.000490 (0.454432) | 0.015386 / 0.000200 (0.015186) | 0.000129 / 0.000054 (0.000074) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.033804 / 0.037411 (-0.003607) | 0.099317 / 0.014526 (0.084791) | 0.107207 / 0.176557 (-0.069349) | 0.160926 / 0.737135 (-0.576210) | 0.108669 / 0.296338 (-0.187670) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.430776 / 0.215209 (0.215567) | 4.297622 / 2.077655 (2.219967) | 2.285918 / 1.504120 (0.781798) | 2.109608 / 1.541195 (0.568413) | 2.208326 / 1.468490 (0.739836) | 0.490016 / 4.584777 (-4.094761) | 3.570609 / 3.745712 (-0.175103) | 3.406335 / 5.269862 (-1.863526) | 2.070664 / 4.565676 (-2.495012) | 0.058089 / 0.424275 (-0.366186) | 0.007425 / 0.007607 (-0.000182) | 0.506972 / 0.226044 (0.280927) | 5.078643 / 2.268929 (2.809714) | 2.858973 / 55.444624 (-52.585651) | 2.457344 / 6.876477 (-4.419132) | 2.687727 / 2.142072 (0.545654) | 0.592134 / 4.805227 (-4.213093) | 0.133966 / 6.500664 (-6.366698) | 0.061800 / 0.075469 (-0.013669) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 1.337167 / 1.841788 (-0.504620) | 20.743951 / 8.074308 (12.669643) | 15.402686 / 10.191392 (5.211294) | 0.164548 / 0.680424 (-0.515876) | 0.020244 / 0.534201 (-0.513957) | 0.399044 / 0.579283 (-0.180239) | 0.414036 / 0.434364 (-0.020328) | 0.474141 / 0.540337 (-0.066197) | 0.654455 / 1.386936 (-0.732482) |\\n\\n</details>\\n</details>\\n\\n![](https://cml.dev/watermark.png#4de930c45a81a6dff1805bf45f59170e9f953eeb \"CML watermark\")\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_count['comments_data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6db6ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71167047343e4d83896575e56fce72c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'body', 'html_url', 'comments', 'number', 'comments_data', 'comments_count', 'text'],\n",
       "    num_rows: 1728\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        +  examples[\"comments_data\"]\n",
    "    }\n",
    "\n",
    "commented_df = dataset_count.map (concatenate_text)\n",
    "commented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ba515c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fix checking patterns to infer packaged builder  \\n Don\\'t ignore results of pattern resolving if `self.data_files` is not None. Otherwise lines 854 and 1037 make no sense. \\n <details>\\n<summary>Show benchmarks</summary>\\n\\nPyArrow==8.0.0\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.006681 / 0.011353 (-0.004672) | 0.003967 / 0.011008 (-0.007041) | 0.085590 / 0.038508 (0.047082) | 0.079285 / 0.023109 (0.056176) | 0.311583 / 0.275898 (0.035685) | 0.345578 / 0.323480 (0.022098) | 0.004115 / 0.007986 (-0.003871) | 0.004286 / 0.004328 (-0.000043) | 0.064405 / 0.004250 (0.060155) | 0.055084 / 0.037052 (0.018032) | 0.316117 / 0.258489 (0.057628) | 0.354737 / 0.293841 (0.060896) | 0.031280 / 0.128546 (-0.097266) | 0.008395 / 0.075646 (-0.067251) | 0.288910 / 0.419271 (-0.130362) | 0.051291 / 0.043533 (0.007759) | 0.309125 / 0.255139 (0.053986) | 0.349673 / 0.283200 (0.066473) | 0.025016 / 0.141683 (-0.116667) | 1.475577 / 1.452155 (0.023422) | 1.558967 / 1.492716 (0.066251) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.208504 / 0.018006 (0.190498) | 0.462270 / 0.000490 (0.461780) | 0.003476 / 0.000200 (0.003276) | 0.000073 / 0.000054 (0.000018) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.030371 / 0.037411 (-0.007041) | 0.086157 / 0.014526 (0.071631) | 0.098162 / 0.176557 (-0.078395) | 0.154649 / 0.737135 (-0.582486) | 0.098697 / 0.296338 (-0.197642) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.405883 / 0.215209 (0.190674) | 4.049614 / 2.077655 (1.971959) | 2.075047 / 1.504120 (0.570927) | 1.917782 / 1.541195 (0.376587) | 2.030268 / 1.468490 (0.561778) | 0.483974 / 4.584777 (-4.100803) | 3.542147 / 3.745712 (-0.203566) | 3.305999 / 5.269862 (-1.963863) | 2.052287 / 4.565676 (-2.513390) | 0.057246 / 0.424275 (-0.367029) | 0.007631 / 0.007607 (0.000024) | 0.488189 / 0.226044 (0.262144) | 4.884784 / 2.268929 (2.615856) | 2.576304 / 55.444624 (-52.868320) | 2.241249 / 6.876477 (-4.635228) | 2.490512 / 2.142072 (0.348440) | 0.584495 / 4.805227 (-4.220733) | 0.134741 / 6.500664 (-6.365923) | 0.061639 / 0.075469 (-0.013830) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 1.317717 / 1.841788 (-0.524071) | 20.098594 / 8.074308 (12.024286) | 14.641051 / 10.191392 (4.449659) | 0.165291 / 0.680424 (-0.515133) | 0.019179 / 0.534201 (-0.515022) | 0.399506 / 0.579283 (-0.179777) | 0.407662 / 0.434364 (-0.026701) | 0.457965 / 0.540337 (-0.082372) | 0.626401 / 1.386936 (-0.760536) |\\n\\n</details>\\nPyArrow==latest\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.007076 / 0.011353 (-0.004277) | 0.004125 / 0.011008 (-0.006884) | 0.064861 / 0.038508 (0.026353) | 0.082390 / 0.023109 (0.059281) | 0.423227 / 0.275898 (0.147329) | 0.452229 / 0.323480 (0.128750) | 0.005594 / 0.007986 (-0.002392) | 0.003465 / 0.004328 (-0.000863) | 0.064661 / 0.004250 (0.060411) | 0.057945 / 0.037052 (0.020892) | 0.424572 / 0.258489 (0.166083) | 0.465349 / 0.293841 (0.171509) | 0.032687 / 0.128546 (-0.095859) | 0.008573 / 0.075646 (-0.067074) | 0.073020 / 0.419271 (-0.346251) | 0.048423 / 0.043533 (0.004891) | 0.413425 / 0.255139 (0.158286) | 0.433778 / 0.283200 (0.150578) | 0.023942 / 0.141683 (-0.117741) | 1.495190 / 1.452155 (0.043036) | 1.586526 / 1.492716 (0.093810) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.271805 / 0.018006 (0.253799) | 0.454922 / 0.000490 (0.454432) | 0.015386 / 0.000200 (0.015186) | 0.000129 / 0.000054 (0.000074) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.033804 / 0.037411 (-0.003607) | 0.099317 / 0.014526 (0.084791) | 0.107207 / 0.176557 (-0.069349) | 0.160926 / 0.737135 (-0.576210) | 0.108669 / 0.296338 (-0.187670) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.430776 / 0.215209 (0.215567) | 4.297622 / 2.077655 (2.219967) | 2.285918 / 1.504120 (0.781798) | 2.109608 / 1.541195 (0.568413) | 2.208326 / 1.468490 (0.739836) | 0.490016 / 4.584777 (-4.094761) | 3.570609 / 3.745712 (-0.175103) | 3.406335 / 5.269862 (-1.863526) | 2.070664 / 4.565676 (-2.495012) | 0.058089 / 0.424275 (-0.366186) | 0.007425 / 0.007607 (-0.000182) | 0.506972 / 0.226044 (0.280927) | 5.078643 / 2.268929 (2.809714) | 2.858973 / 55.444624 (-52.585651) | 2.457344 / 6.876477 (-4.419132) | 2.687727 / 2.142072 (0.545654) | 0.592134 / 4.805227 (-4.213093) | 0.133966 / 6.500664 (-6.366698) | 0.061800 / 0.075469 (-0.013669) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 1.337167 / 1.841788 (-0.504620) | 20.743951 / 8.074308 (12.669643) | 15.402686 / 10.191392 (5.211294) | 0.164548 / 0.680424 (-0.515876) | 0.020244 / 0.534201 (-0.513957) | 0.399044 / 0.579283 (-0.180239) | 0.414036 / 0.434364 (-0.020328) | 0.474141 / 0.540337 (-0.066197) | 0.654455 / 1.386936 (-0.732482) |\\n\\n</details>\\n</details>\\n\\n![](https://cml.dev/watermark.png#4de930c45a81a6dff1805bf45f59170e9f953eeb \"CML watermark\")\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commented_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f9b23afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8a73030c324ee6bd7dac4b2c82a42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Z004RJZU\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83432464b6741e8853336fbbd00056e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c1176766e2420ca1c4392ffcc2d28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6ce905ddf3452bb10ba35e1ceb6bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc6b4849f8647f5b3a9cb57225ad220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4ffe70a09f418abf12effb6e1649b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e04ee036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "292a73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e83c1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(commented_df[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9298e367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a958db13d648dab6e85504be41527d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "You must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. A community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. Note that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m embeddings_dataset \u001b[39m=\u001b[39m commented_df\u001b[39m.\u001b[39mmap(\n\u001b[0;32m      2\u001b[0m     \u001b[39mlambda\u001b[39;00m x: {\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m: get_embeddings(x[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]}\n\u001b[0;32m      3\u001b[0m )\n\u001b[1;32m----> 5\u001b[0m embeddings_dataset\u001b[39m.\u001b[39;49madd_faiss_index(column\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39membeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\arrow_dataset.py:5693\u001b[0m, in \u001b[0;36mDataset.add_faiss_index\u001b[1;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose, dtype)\u001b[0m\n\u001b[0;32m   5639\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Add a dense index using Faiss for fast retrieval.\u001b[39;00m\n\u001b[0;32m   5640\u001b[0m \u001b[39mBy default the index is done over the vectors of the specified column.\u001b[39;00m\n\u001b[0;32m   5641\u001b[0m \u001b[39mYou can specify `device` if you want to run it on GPU (`device` must be the GPU index).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5690\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[0;32m   5691\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5692\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformatted_as(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, columns\u001b[39m=\u001b[39m[column], dtype\u001b[39m=\u001b[39mdtype):\n\u001b[1;32m-> 5693\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madd_faiss_index(\n\u001b[0;32m   5694\u001b[0m         column\u001b[39m=\u001b[39;49mcolumn,\n\u001b[0;32m   5695\u001b[0m         index_name\u001b[39m=\u001b[39;49mindex_name,\n\u001b[0;32m   5696\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m   5697\u001b[0m         string_factory\u001b[39m=\u001b[39;49mstring_factory,\n\u001b[0;32m   5698\u001b[0m         metric_type\u001b[39m=\u001b[39;49mmetric_type,\n\u001b[0;32m   5699\u001b[0m         custom_index\u001b[39m=\u001b[39;49mcustom_index,\n\u001b[0;32m   5700\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   5701\u001b[0m         train_size\u001b[39m=\u001b[39;49mtrain_size,\n\u001b[0;32m   5702\u001b[0m         faiss_verbose\u001b[39m=\u001b[39;49mfaiss_verbose,\n\u001b[0;32m   5703\u001b[0m     )\n\u001b[0;32m   5704\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\search.py:480\u001b[0m, in \u001b[0;36mIndexableMixin.add_faiss_index\u001b[1;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Add a dense index using Faiss for fast retrieval.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39mThe index is created using the vectors of the specified column.\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[39mYou can specify `device` if you want to run it on GPU (`device` must be the GPU index, see more below).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39m    faiss_verbose (`bool`, defaults to False): Enable the verbosity of the Faiss index.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    479\u001b[0m index_name \u001b[39m=\u001b[39m index_name \u001b[39mif\u001b[39;00m index_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m column\n\u001b[1;32m--> 480\u001b[0m faiss_index \u001b[39m=\u001b[39m FaissIndex(\n\u001b[0;32m    481\u001b[0m     device\u001b[39m=\u001b[39;49mdevice, string_factory\u001b[39m=\u001b[39;49mstring_factory, metric_type\u001b[39m=\u001b[39;49mmetric_type, custom_index\u001b[39m=\u001b[39;49mcustom_index\n\u001b[0;32m    482\u001b[0m )\n\u001b[0;32m    483\u001b[0m faiss_index\u001b[39m.\u001b[39madd_vectors(\n\u001b[0;32m    484\u001b[0m     \u001b[39mself\u001b[39m, column\u001b[39m=\u001b[39mcolumn, batch_size\u001b[39m=\u001b[39mbatch_size, train_size\u001b[39m=\u001b[39mtrain_size, faiss_verbose\u001b[39m=\u001b[39mfaiss_verbose\n\u001b[0;32m    485\u001b[0m )\n\u001b[0;32m    486\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indexes[index_name] \u001b[39m=\u001b[39m faiss_index\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\search.py:247\u001b[0m, in \u001b[0;36mFaissIndex.__init__\u001b[1;34m(self, device, string_factory, metric_type, custom_index)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfaiss_index \u001b[39m=\u001b[39m custom_index\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_faiss:\n\u001b[1;32m--> 247\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNote that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    251\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: You must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. A community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. Note that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available."
     ]
    }
   ],
   "source": [
    "embeddings_dataset = commented_df.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")\n",
    "\n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
