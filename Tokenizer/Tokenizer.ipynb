{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a03329",
   "metadata": {},
   "source": [
    "## Training a new Tokenizer from an old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813e6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09f6523",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset ('code_search_net','python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2e17c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repository_name': 'ageitgey/face_recognition',\n",
       " 'func_path_in_repository': 'examples/face_recognition_knn.py',\n",
       " 'func_name': 'train',\n",
       " 'whole_func_string': 'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf',\n",
       " 'language': 'python',\n",
       " 'func_code_string': 'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf',\n",
       " 'func_code_tokens': ['def',\n",
       "  'train',\n",
       "  '(',\n",
       "  'train_dir',\n",
       "  ',',\n",
       "  'model_save_path',\n",
       "  '=',\n",
       "  'None',\n",
       "  ',',\n",
       "  'n_neighbors',\n",
       "  '=',\n",
       "  'None',\n",
       "  ',',\n",
       "  'knn_algo',\n",
       "  '=',\n",
       "  \"'ball_tree'\",\n",
       "  ',',\n",
       "  'verbose',\n",
       "  '=',\n",
       "  'False',\n",
       "  ')',\n",
       "  ':',\n",
       "  'X',\n",
       "  '=',\n",
       "  '[',\n",
       "  ']',\n",
       "  'y',\n",
       "  '=',\n",
       "  '[',\n",
       "  ']',\n",
       "  '# Loop through each person in the training set',\n",
       "  'for',\n",
       "  'class_dir',\n",
       "  'in',\n",
       "  'os',\n",
       "  '.',\n",
       "  'listdir',\n",
       "  '(',\n",
       "  'train_dir',\n",
       "  ')',\n",
       "  ':',\n",
       "  'if',\n",
       "  'not',\n",
       "  'os',\n",
       "  '.',\n",
       "  'path',\n",
       "  '.',\n",
       "  'isdir',\n",
       "  '(',\n",
       "  'os',\n",
       "  '.',\n",
       "  'path',\n",
       "  '.',\n",
       "  'join',\n",
       "  '(',\n",
       "  'train_dir',\n",
       "  ',',\n",
       "  'class_dir',\n",
       "  ')',\n",
       "  ')',\n",
       "  ':',\n",
       "  'continue',\n",
       "  '# Loop through each training image for the current person',\n",
       "  'for',\n",
       "  'img_path',\n",
       "  'in',\n",
       "  'image_files_in_folder',\n",
       "  '(',\n",
       "  'os',\n",
       "  '.',\n",
       "  'path',\n",
       "  '.',\n",
       "  'join',\n",
       "  '(',\n",
       "  'train_dir',\n",
       "  ',',\n",
       "  'class_dir',\n",
       "  ')',\n",
       "  ')',\n",
       "  ':',\n",
       "  'image',\n",
       "  '=',\n",
       "  'face_recognition',\n",
       "  '.',\n",
       "  'load_image_file',\n",
       "  '(',\n",
       "  'img_path',\n",
       "  ')',\n",
       "  'face_bounding_boxes',\n",
       "  '=',\n",
       "  'face_recognition',\n",
       "  '.',\n",
       "  'face_locations',\n",
       "  '(',\n",
       "  'image',\n",
       "  ')',\n",
       "  'if',\n",
       "  'len',\n",
       "  '(',\n",
       "  'face_bounding_boxes',\n",
       "  ')',\n",
       "  '!=',\n",
       "  '1',\n",
       "  ':',\n",
       "  '# If there are no people (or too many people) in a training image, skip the image.',\n",
       "  'if',\n",
       "  'verbose',\n",
       "  ':',\n",
       "  'print',\n",
       "  '(',\n",
       "  '\"Image {} not suitable for training: {}\"',\n",
       "  '.',\n",
       "  'format',\n",
       "  '(',\n",
       "  'img_path',\n",
       "  ',',\n",
       "  '\"Didn\\'t find a face\"',\n",
       "  'if',\n",
       "  'len',\n",
       "  '(',\n",
       "  'face_bounding_boxes',\n",
       "  ')',\n",
       "  '<',\n",
       "  '1',\n",
       "  'else',\n",
       "  '\"Found more than one face\"',\n",
       "  ')',\n",
       "  ')',\n",
       "  'else',\n",
       "  ':',\n",
       "  '# Add face encoding for current image to the training set',\n",
       "  'X',\n",
       "  '.',\n",
       "  'append',\n",
       "  '(',\n",
       "  'face_recognition',\n",
       "  '.',\n",
       "  'face_encodings',\n",
       "  '(',\n",
       "  'image',\n",
       "  ',',\n",
       "  'known_face_locations',\n",
       "  '=',\n",
       "  'face_bounding_boxes',\n",
       "  ')',\n",
       "  '[',\n",
       "  '0',\n",
       "  ']',\n",
       "  ')',\n",
       "  'y',\n",
       "  '.',\n",
       "  'append',\n",
       "  '(',\n",
       "  'class_dir',\n",
       "  ')',\n",
       "  '# Determine how many neighbors to use for weighting in the KNN classifier',\n",
       "  'if',\n",
       "  'n_neighbors',\n",
       "  'is',\n",
       "  'None',\n",
       "  ':',\n",
       "  'n_neighbors',\n",
       "  '=',\n",
       "  'int',\n",
       "  '(',\n",
       "  'round',\n",
       "  '(',\n",
       "  'math',\n",
       "  '.',\n",
       "  'sqrt',\n",
       "  '(',\n",
       "  'len',\n",
       "  '(',\n",
       "  'X',\n",
       "  ')',\n",
       "  ')',\n",
       "  ')',\n",
       "  ')',\n",
       "  'if',\n",
       "  'verbose',\n",
       "  ':',\n",
       "  'print',\n",
       "  '(',\n",
       "  '\"Chose n_neighbors automatically:\"',\n",
       "  ',',\n",
       "  'n_neighbors',\n",
       "  ')',\n",
       "  '# Create and train the KNN classifier',\n",
       "  'knn_clf',\n",
       "  '=',\n",
       "  'neighbors',\n",
       "  '.',\n",
       "  'KNeighborsClassifier',\n",
       "  '(',\n",
       "  'n_neighbors',\n",
       "  '=',\n",
       "  'n_neighbors',\n",
       "  ',',\n",
       "  'algorithm',\n",
       "  '=',\n",
       "  'knn_algo',\n",
       "  ',',\n",
       "  'weights',\n",
       "  '=',\n",
       "  \"'distance'\",\n",
       "  ')',\n",
       "  'knn_clf',\n",
       "  '.',\n",
       "  'fit',\n",
       "  '(',\n",
       "  'X',\n",
       "  ',',\n",
       "  'y',\n",
       "  ')',\n",
       "  '# Save the trained KNN classifier',\n",
       "  'if',\n",
       "  'model_save_path',\n",
       "  'is',\n",
       "  'not',\n",
       "  'None',\n",
       "  ':',\n",
       "  'with',\n",
       "  'open',\n",
       "  '(',\n",
       "  'model_save_path',\n",
       "  ',',\n",
       "  \"'wb'\",\n",
       "  ')',\n",
       "  'as',\n",
       "  'f',\n",
       "  ':',\n",
       "  'pickle',\n",
       "  '.',\n",
       "  'dump',\n",
       "  '(',\n",
       "  'knn_clf',\n",
       "  ',',\n",
       "  'f',\n",
       "  ')',\n",
       "  'return',\n",
       "  'knn_clf'],\n",
       " 'func_documentation_string': 'Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.',\n",
       " 'func_documentation_tokens': ['Trains',\n",
       "  'a',\n",
       "  'k',\n",
       "  '-',\n",
       "  'nearest',\n",
       "  'neighbors',\n",
       "  'classifier',\n",
       "  'for',\n",
       "  'face',\n",
       "  'recognition',\n",
       "  '.'],\n",
       " 'split_name': 'train',\n",
       " 'func_code_url': 'https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L46-L108'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = dataset.copy()\n",
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cddda8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")\n",
    "print (len (next(iter(training_corpus))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36fd987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "raw_datasets_1 = load_dataset ('code_search_net','python' , split='train' ,streaming=True)\n",
    "print (len (next(iter(raw_datasets_1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74554b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object square_generator at 0x0000020127FFE190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square_generator(n):\n",
    "    for i in range(n):\n",
    "        yield i ** 2\n",
    "\n",
    "# Using the generator\n",
    "gen = square_generator(5)\n",
    "for square in gen:\n",
    "    print(square)\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd795aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475d9bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "old_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab87d08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'n',\n",
       " 'umbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`',\n",
       " '.\"',\n",
       " '\"\"',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db4c54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4dd9d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=52000, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb010a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=52000, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_2 = old_tokenizer.train_new_from_iterator(raw_datasets[\"train\"][\"whole_func_string\"], 52000)\n",
    "tokenizer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275953e0",
   "metadata": {},
   "source": [
    "Lets check that what our new tokeizer have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6550ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "610ccda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "old token len = 36\n",
      "new token len = 27\n",
      "new token2 len = 27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (f\"\"\"\n",
    "old token len = {len (old_tokenizer.tokenize(example))}\n",
    "new token len = {len (tokenizer.tokenize(example))}\n",
    "new token2 len = {len (tokenizer_2.tokenize(example))}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf012546",
   "metadata": {},
   "source": [
    "## Fast tokenizers' special powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97799492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11a26773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7f8380f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'S',\n",
       " '##yl',\n",
       " '##va',\n",
       " '##in',\n",
       " 'and',\n",
       " 'I',\n",
       " 'work',\n",
       " 'at',\n",
       " 'Hu',\n",
       " '##gging',\n",
       " 'Face',\n",
       " 'in',\n",
       " 'Brooklyn',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d46659f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "479a25ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52a70101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '81', '##s', '[SEP]']   ['81', 's']\n"
     ]
    }
   ],
   "source": [
    "checkpoint_bert = \"bert-base-cased\" \n",
    "checkpoint_gpt2 = \"gpt2\"\n",
    "\n",
    "tokenizer_b = AutoTokenizer.from_pretrained (checkpoint_bert)\n",
    "tokenizer_g = AutoTokenizer.from_pretrained (checkpoint_gpt2)\n",
    "\n",
    "test_sen = '81s'\n",
    "\n",
    "print (tokenizer_b(test_sen).tokens(), \" \" , tokenizer_g(test_sen).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aea23b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Face'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(9)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff7f6193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b045f784e780401eb580a10df3793ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\z004rjzu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Z004RJZU\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149fcdd7a04e457da73534313005f689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e57b22b85794c159142b09c2790f7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6049b8513ed2440685912b78e470109b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99938285,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99815494,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99590707,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99923277,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.976115,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887976,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9932106,\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6992fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99938285,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99815494,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99590707,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99923277,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.976115,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887976,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9932106,\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tocken_classifier = pipeline (\"token-classification\", aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adb3bf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a768a261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19, 9])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f76c8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = outputs.logits.argmax(dim = -1)[0].tolist()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fb27b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9994322657585144,\n",
       "  1.6470316040795296e-05,\n",
       "  3.426706462050788e-05,\n",
       "  1.6042342394939624e-05,\n",
       "  8.250699465861544e-05,\n",
       "  2.1382335035013966e-05,\n",
       "  0.00015649119450245053,\n",
       "  1.9652115952339955e-05,\n",
       "  0.0002208926307503134],\n",
       " [0.9989632368087769,\n",
       "  1.8515793271944858e-05,\n",
       "  5.240468090050854e-05,\n",
       "  1.2534746019809972e-05,\n",
       "  0.0004347375070210546,\n",
       "  3.087438744842075e-05,\n",
       "  0.0003146881645079702,\n",
       "  2.7860729460371658e-05,\n",
       "  0.00014510894834529608],\n",
       " [0.9997084736824036,\n",
       "  8.308127689815592e-06,\n",
       "  2.874564415833447e-05,\n",
       "  5.650359071296407e-06,\n",
       "  8.694865391589701e-05,\n",
       "  9.783468158275355e-06,\n",
       "  6.786145968362689e-05,\n",
       "  1.179398168460466e-05,\n",
       "  7.241901766974479e-05],\n",
       " [0.9998351335525513,\n",
       "  5.645516012009466e-06,\n",
       "  1.3955113900010474e-05,\n",
       "  4.3133613871759735e-06,\n",
       "  4.017679748358205e-05,\n",
       "  8.123048246488906e-06,\n",
       "  5.6484808737877756e-05,\n",
       "  8.991601134766825e-06,\n",
       "  2.7239038900006562e-05],\n",
       " [0.0001833340502344072,\n",
       "  2.515659434720874e-05,\n",
       "  4.8462032282259315e-05,\n",
       "  1.4900567293807399e-05,\n",
       "  0.9993828535079956,\n",
       "  1.999772030103486e-05,\n",
       "  0.00011153631203342229,\n",
       "  1.0790749911393505e-05,\n",
       "  0.00020288894302211702],\n",
       " [0.0006440277211368084,\n",
       "  7.437902240781114e-05,\n",
       "  0.00013196632789913565,\n",
       "  3.471966920187697e-05,\n",
       "  0.9981549382209778,\n",
       "  3.3829721360234544e-05,\n",
       "  0.0005438181106001139,\n",
       "  1.9978198906756006e-05,\n",
       "  0.000362446706276387],\n",
       " [0.001640839851461351,\n",
       "  9.469484211876988e-05,\n",
       "  0.00027364378911443055,\n",
       "  4.440645716385916e-05,\n",
       "  0.995907187461853,\n",
       "  5.1262213673908263e-05,\n",
       "  0.0012787932064384222,\n",
       "  3.2835454476298764e-05,\n",
       "  0.0006763276760466397],\n",
       " [0.0002290184493176639,\n",
       "  2.5183346224366687e-05,\n",
       "  5.789952410850674e-05,\n",
       "  9.957029760698788e-06,\n",
       "  0.9992327690124512,\n",
       "  1.7655091141932644e-05,\n",
       "  0.00023448391584679484,\n",
       "  1.235660420206841e-05,\n",
       "  0.00018065057520288974],\n",
       " [0.999804675579071,\n",
       "  5.465042249852559e-06,\n",
       "  1.2950016753165983e-05,\n",
       "  4.9724390009942e-06,\n",
       "  2.350325303268619e-05,\n",
       "  1.2930728189530782e-05,\n",
       "  9.509925439488143e-05,\n",
       "  8.891763172869105e-06,\n",
       "  3.1469367968384176e-05],\n",
       " [0.9995046854019165,\n",
       "  1.4611873666581232e-05,\n",
       "  2.9646907933056355e-05,\n",
       "  8.223405529861338e-06,\n",
       "  0.0001601653202669695,\n",
       "  2.045680594164878e-05,\n",
       "  0.00017537118401378393,\n",
       "  1.9349117792444304e-05,\n",
       "  6.743980338796973e-05],\n",
       " [0.9996775388717651,\n",
       "  7.5969492172589526e-06,\n",
       "  1.7006925190798938e-05,\n",
       "  3.777614210775937e-06,\n",
       "  6.396068056346849e-05,\n",
       "  1.2297865396249108e-05,\n",
       "  0.00018241393263451755,\n",
       "  7.648402970517054e-06,\n",
       "  2.7664631488732994e-05],\n",
       " [0.999434769153595,\n",
       "  1.1278328202024568e-05,\n",
       "  2.8862716135336086e-05,\n",
       "  6.246603334147949e-06,\n",
       "  8.598279964644462e-05,\n",
       "  2.298940671607852e-05,\n",
       "  0.00034945228253491223,\n",
       "  1.3841326108376961e-05,\n",
       "  4.654669464798644e-05],\n",
       " [0.018156297504901886,\n",
       "  6.245910481084138e-05,\n",
       "  0.00026932216132991016,\n",
       "  4.766615529661067e-05,\n",
       "  0.006134612485766411,\n",
       "  0.00023967419110704213,\n",
       "  0.9738931059837341,\n",
       "  7.093788008205593e-05,\n",
       "  0.0011259125312790275],\n",
       " [0.014645997434854507,\n",
       "  0.00020479796512518078,\n",
       "  0.00223603006452322,\n",
       "  9.35708376346156e-05,\n",
       "  0.003731631673872471,\n",
       "  0.0005988673656247556,\n",
       "  0.9761149883270264,\n",
       "  0.00017609840142540634,\n",
       "  0.0021980952005833387],\n",
       " [0.003171562682837248,\n",
       "  8.892165351426229e-05,\n",
       "  0.0015001439023762941,\n",
       "  7.653006468899548e-05,\n",
       "  0.003357557114213705,\n",
       "  0.0004643824358936399,\n",
       "  0.9887976050376892,\n",
       "  0.00010871328413486481,\n",
       "  0.0024345917627215385],\n",
       " [0.9995326995849609,\n",
       "  6.5539725255803205e-06,\n",
       "  2.831619895005133e-05,\n",
       "  6.235947694221977e-06,\n",
       "  3.7372108636191115e-05,\n",
       "  2.0357188986963592e-05,\n",
       "  0.00028727558674290776,\n",
       "  1.5032655028335284e-05,\n",
       "  6.614311132580042e-05],\n",
       " [0.0006589232361875474,\n",
       "  6.671254232060164e-05,\n",
       "  0.00022443989291787148,\n",
       "  4.190396794001572e-05,\n",
       "  0.000460203766124323,\n",
       "  9.038800635607913e-05,\n",
       "  0.005088851787149906,\n",
       "  0.0001580321986693889,\n",
       "  0.9932106137275696],\n",
       " [0.9994321465492249,\n",
       "  1.647070712351706e-05,\n",
       "  3.426768307690509e-05,\n",
       "  1.604247881914489e-05,\n",
       "  8.250911196228117e-05,\n",
       "  2.138237323379144e-05,\n",
       "  0.00015649358101654798,\n",
       "  1.9652299670269713e-05,\n",
       "  0.00022089408594183624],\n",
       " [0.9994322657585144,\n",
       "  1.6470299669890665e-05,\n",
       "  3.426706462050788e-05,\n",
       "  1.6042327843024395e-05,\n",
       "  8.250691462308168e-05,\n",
       "  2.1382315026130527e-05,\n",
       "  0.00015649090346414596,\n",
       "  1.965215415111743e-05,\n",
       "  0.0002208924270235002]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21b5cb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f60fe52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'}\n",
      "{'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl'}\n",
      "{'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'}\n",
      "{'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'}\n",
      "{'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'}\n",
      "{'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'}\n",
      "{'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face'}\n",
      "{'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn'}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        results.append(\n",
    "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
    "        )\n",
    "\n",
    "for results in results:\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f89efc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 14),\n",
       " (14, 16),\n",
       " (16, 18),\n",
       " (19, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 32),\n",
       " (33, 35),\n",
       " (35, 40),\n",
       " (41, 45),\n",
       " (46, 48),\n",
       " (49, 57),\n",
       " (57, 58),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "inputs_with_offsets[\"offset_mapping\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa093048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S', 'start': 11, 'end': 12}\n",
      "{'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl', 'start': 12, 'end': 14}\n",
      "{'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va', 'start': 14, 'end': 16}\n",
      "{'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in', 'start': 16, 'end': 18}\n",
      "{'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu', 'start': 33, 'end': 35}\n",
      "{'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging', 'start': 35, 'end': 40}\n",
      "{'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face', 'start': 41, 'end': 45}\n",
      "{'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "for results in results:\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "053b2e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'PER', 'score': 0.9981694370508194, 'word': 'Sylvain', 'start': 11, 'end': 18}\n",
      "{'entity_group': 'ORG', 'score': 0.9796018997828165, 'word': 'Hugging Face', 'start': 33, 'end': 45}\n",
      "{'entity_group': 'LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "for results in results:\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008bc54d",
   "metadata": {},
   "source": [
    "# Normalizing and pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf235093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93601d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"olla, My name i#s safeer, Héllò hôw are ü?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "81a16719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olla, my name i#s safeer, hello how are u?'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.normalizer.normalize_str (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "938f4780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olla, My name i#s safeer, Héllò hôw are ü?'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint1 = 'bert-base-cased'\n",
    "tokenizer1 = AutoTokenizer.from_pretrained (checkpoint1)\n",
    "tokenizer1.backend_tokenizer.normalizer.normalize_str (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b8044f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1106b7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ġhow', (6, 10)),\n",
       " ('Ġare', (10, 14)),\n",
       " ('Ġ', (14, 15)),\n",
       " ('Ġyou', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d083b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02087692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e2d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1afdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
